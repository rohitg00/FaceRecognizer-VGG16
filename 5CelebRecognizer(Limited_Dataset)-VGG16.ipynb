{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding VGG16 on base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16 was designed to work on 224 x 224 pixel input images sizes\n",
    "img_rows = 224\n",
    "img_cols = 224 \n",
    "\n",
    "#include_top=False removes the output layer of the model\n",
    "base_model = VGG16(weights = 'imagenet', \n",
    "                 include_top = False, \n",
    "                 input_shape = (img_rows, img_cols, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To get only the name\n",
    "base_model.layers[0].__class__.__name__\n",
    "\n",
    "#To see the input/output of a particular layer\n",
    "base_model.layers[0].input\n",
    "\n",
    "#Freezing all the layers by making their trainable=False\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable=False\n",
    "    \n",
    "#Checking this\n",
    "base_model.layers[12].trainable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 InputLayer False\n",
      "1 Conv2D False\n",
      "2 Conv2D False\n",
      "3 MaxPooling2D False\n",
      "4 Conv2D False\n",
      "5 Conv2D False\n",
      "6 MaxPooling2D False\n",
      "7 Conv2D False\n",
      "8 Conv2D False\n",
      "9 Conv2D False\n",
      "10 MaxPooling2D False\n",
      "11 Conv2D False\n",
      "12 Conv2D False\n",
      "13 Conv2D False\n",
      "14 MaxPooling2D False\n",
      "15 Conv2D False\n",
      "16 Conv2D False\n",
      "17 Conv2D False\n",
      "18 MaxPooling2D False\n"
     ]
    }
   ],
   "source": [
    "# Let's print our layers \n",
    "for (i,layer) in enumerate(base_model.layers):\n",
    "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'block5_pool/MaxPool:0' shape=(None, 7, 7, 512) dtype=float32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ouput of the current model\n",
    "base_model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we add our dense layers for a new prediction on top of the base model\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "\n",
    "top_model = base_model.output\n",
    "top_model = Flatten()(top_model)\n",
    "top_model = Dense(512, activation='relu')(top_model)   #First added FCL dense layer\n",
    "top_model = Dense(512, activation='relu')(top_model)    #Second added FCL dense layer\n",
    "top_model = Dense(256, activation='relu')(top_model)    #Third added FCL dense layer\n",
    "top_model = Dense(5, activation='softmax')(top_model)    #Output layer with 2 class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_4/Softmax:0' shape=(None, 5) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now let's see the top_model output\n",
    "top_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_1:0' shape=(None, 224, 224, 3) dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMP: Mounting the base_model with the top_model and forming newmodel\n",
    "\n",
    "from keras.models import Model\n",
    "newmodel = Model(inputs=base_model.input, outputs=top_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_4/Softmax:0' shape=(None, 5) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newmodel.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x1ea6fe27508>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1ea6fe331c8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1ea6b47cbc8>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x1ea6fe5b0c8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1ea6fe91e48>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1ea6fea2748>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x1ea6fea6308>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1ea6fead888>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1ea6fec0fc8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1ea6fed3088>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x1ea6fee76c8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1ea6fef1a48>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1ea6ff06948>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1ea6ff0ff08>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x1ea6ff23748>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1ea6ff28b08>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1ea6ff4a2c8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1ea6ff4fa48>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x1ea6ff64a88>,\n",
       " <keras.layers.core.Flatten at 0x1ea6ffb41c8>,\n",
       " <keras.layers.core.Dense at 0x1ea6ffb4208>,\n",
       " <keras.layers.core.Dense at 0x1ea6ffb4548>,\n",
       " <keras.layers.core.Dense at 0x1ea6ffb9ec8>,\n",
       " <keras.layers.core.Dense at 0x1ea70102d08>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newmodel.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 27,955,525\n",
      "Trainable params: 13,240,837\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "newmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 93 images belonging to 5 classes.\n",
      "Found 25 images belonging to 5 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#Converting to 4D\\nfor img_test in test_generator:\\n    img_test=np.expand_dims(img_test, axis=0)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing our images for recognition\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_data=\"\\\\Users/ROHIT/OneDrive/Desktop/mlops/Transfer Learning for Face-Recog/train/\"\n",
    "test_data=\"\\\\Users/ROHIT/OneDrive/Desktop/mlops/Transfer Learning for Face-Recog/validate/\"\n",
    "\n",
    "#Data image augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=20,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    " \n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        class_mode='categorical')\n",
    "\n",
    "'''#Converting to 4D\n",
    "for img_train in train_generator:\n",
    "    img_train=np.expand_dims(img_train, axis=0)'''\n",
    " \n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_data,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "'''#Converting to 4D\n",
    "for img_test in test_generator:\n",
    "    img_test=np.expand_dims(img_test, axis=0)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's compile our model\n",
    "from keras.optimizers import RMSprop\n",
    "newmodel.compile(optimizer = RMSprop(lr=0.0001),\n",
    "                 loss = 'categorical_crossentropy',\n",
    "                 metrics =['accuracy']\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "50/50 [==============================] - 1034s 21s/step - loss: 1.1564 - accuracy: 0.5644 - val_loss: 1.0508 - val_accuracy: 0.6000\n",
      "Epoch 2/2\n",
      "50/50 [==============================] - 1060s 21s/step - loss: 0.4242 - accuracy: 0.8642 - val_loss: 0.8769 - val_accuracy: 0.6400\n"
     ]
    }
   ],
   "source": [
    "history = newmodel.fit_generator(train_generator, epochs=2,steps_per_epoch=50, validation_data=test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "newmodel.save('Celebmini_VGG16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "classifier = load_model('Celebmini_VGG16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the Model\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "five_celeb_dict = {\"[0]\": \"ben_afflek\", \n",
    "                      \"[1]\": \"elton_john\",\n",
    "                      \"[2]\": \"jerry_seinfeld\",\n",
    "                      \"[3]\": \"madonna\",\n",
    "                      \"[4]\": \"mindy_kaling\",\n",
    "                     }\n",
    "\n",
    "five_celeb_dict_n = {\"ben_afflek\": \"ben_afflek\", \n",
    "                      \"elton_john\": \"elton_john\",\n",
    "                      \"jerry_seinfeld\": \"jerry_seinfeld\",\n",
    "                      \"madonna\": \"madonna\",\n",
    "                      \"mindy_kaling\": \"mindy_kaling\",\n",
    "                      }\n",
    "\n",
    "def draw_test(name, pred, im):\n",
    "    celeb =five_celeb_dict[str(pred)]\n",
    "    BLACK = [0,0,0]\n",
    "    expanded_image = cv2.copyMakeBorder(im, 80, 0, 0, 100 ,cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    cv2.putText(expanded_image, celeb, (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255), 2)\n",
    "    cv2.imshow(name, expanded_image)\n",
    "\n",
    "def getRandomImage(path):\n",
    "    \"\"\"function loads a random images from a random folder in our test path \"\"\"\n",
    "    folders = list(filter(lambda x: os.path.isdir(os.path.join(path, x)), os.listdir(path)))\n",
    "    random_directory = np.random.randint(0,len(folders))\n",
    "    path_class = folders[random_directory]\n",
    "    print(\"Class - \" + five_celeb_dict_n[str(path_class)])\n",
    "    file_path = path + path_class\n",
    "    file_names = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "    random_file_index = np.random.randint(0,len(file_names))\n",
    "    image_name = file_names[random_file_index]\n",
    "    return cv2.imread(file_path+\"/\"+image_name)    \n",
    "\n",
    "for i in range(0,10):\n",
    "    input_im = getRandomImage(\"\\\\Users/ROHIT/OneDrive/Desktop/mlops/Transfer Learning for Face-Recog/validate/\")\n",
    "    input_original = input_im.copy()\n",
    "    input_original = cv2.resize(input_original, None, fx=0.5, fy=0.5, interpolation = cv2.INTER_LINEAR)\n",
    "    \n",
    "    input_im = cv2.resize(input_im, (224, 224), interpolation = cv2.INTER_LINEAR)\n",
    "    input_im = input_im / 255.\n",
    "    input_im = input_im.reshape(1,224,224,3) \n",
    "    \n",
    "    # Get Prediction\n",
    "    res = np.argmax(classifier.predict(input_im, 1, verbose = 0), axis=1)\n",
    "    \n",
    "    # Show image with predicted class\n",
    "    draw_test(\"Prediction\", res, input_original) \n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
